{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL of the webpage to scrape\n",
    "base_url = 'https://en.wikisource.org/wiki/A_Dictionary_of_the_Sunda_language/'\n",
    "\n",
    "# List of alphabets from A to Z\n",
    "alphabets = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# List to store all scraped words and hyperlinks\n",
    "based_words = []\n",
    "\n",
    "# Iterate through each alphabet\n",
    "for letter in alphabets:\n",
    "    # Construct the URL for the specific alphabet\n",
    "    url = base_url + letter\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all elements with class 'hanging-indent'\n",
    "        indent_elements = soup.find_all(class_='hanging-indent')\n",
    "\n",
    "        # Extract text and hyperlinks from these elements\n",
    "        for selected_element in indent_elements:\n",
    "            for element in selected_element.find_all('p'):\n",
    "                hyperlink = [link.get('href') for link in element.find_all('a')]\n",
    "                based_words.append({\n",
    "                    'word': element.text,\n",
    "                    'hyperlink': hyperlink  \n",
    "                })\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content from {url}\")\n",
    "\n",
    "# Convert based_words list to JSON format\n",
    "json_data = json.dumps(based_words, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print the JSON data (for visualization)\n",
    "print(json_data)\n",
    "\n",
    "# Optionally, save the JSON data to a file\n",
    "with open('based_words_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(based_words, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basa Sunda [ édit ] Kecap barang\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def extract_word_type(url):\n",
    "    element_class = \"mw-content-ltr\"\n",
    "    try:\n",
    "        # Send an HTTP request to fetch the HTML content of the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad response status\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the specific element by its class name\n",
    "        target_element = soup.find('div', {'class': element_class})\n",
    "\n",
    "        if target_element:\n",
    "            # Function to clean individual text strings\n",
    "            def clean_text(text):\n",
    "                # Remove unwanted patterns using regular expressions\n",
    "                cleaned_text = re.sub(r'\\[.*?\\]', '', text)  # Remove text within square brackets and the brackets themselves\n",
    "                cleaned_text = re.sub(r'\\(id\\)', '', cleaned_text)  # Remove specific (id) occurrences\n",
    "                cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "                cleaned_text = cleaned_text.strip()  # Strip leading and trailing spaces\n",
    "                return cleaned_text\n",
    "\n",
    "            # Extract text content only from <span> tags within the target element and clean it\n",
    "            span_texts = [clean_text(span.get_text(strip=True)) for span in target_element.find_all('span')]\n",
    "\n",
    "            # Filter out empty strings and remove text that only contains '*'\n",
    "            cleaned_texts = [text for text in span_texts if text.strip() and text.strip() != '*']\n",
    "\n",
    "            # Join the list of cleaned extracted texts into a single string\n",
    "            cleaned_text = ' '.join(cleaned_texts)\n",
    "            return cleaned_text\n",
    "        else:\n",
    "            return \"Element not found or page structure has changed.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching or parsing HTML: {e}\"\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://su.wiktionary.org/wiki/adi\"\n",
    "\n",
    "cleaned_text = extract_word_type(url)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the <div> with class 'mw-category-group'\n",
    "    category_group_div = soup.find('div', id='mw-pages')\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    # Find all <a> tags under the <div class=\"mw-category-group\">\n",
    "    if category_group_div:\n",
    "        links = category_group_div.find_all('li')\n",
    "\n",
    "        for link in links:\n",
    "            href = \"https://su.wiktionary.org\" + link.find('a').get('href')\n",
    "            title = link.find('a').get('title')\n",
    "            results.append({'word': title, 'link': href})\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_next_page(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    next_page = soup.find('a', {'title': 'Kategori:Kecap basa Sunda', },string= 'kaca salajengna').get('href')\n",
    "    \n",
    "    return next_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block will result in error, please continue to the next block, it will run properly as intended\n",
    "list_url = []\n",
    "\n",
    "root_url = \"https://su.wiktionary.org/wiki/Kategori:Kecap_basa_Sunda\"\n",
    "\n",
    "while get_next_page(root_url) is not None:\n",
    "    list_url.append(root_url)\n",
    "    root_url = get_next_page(root_url)\n",
    "    root_url = \"https://su.wiktionary.org\" + root_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for url in list_url:\n",
    "    word_list.append(scrape_page(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the list\n",
    "word_list = [item for sublist in word_list for item in sublist] \n",
    "\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_link = word_list[0]['link']\n",
    "word_type = extract_word_type(test_link)\n",
    "def clean_word_tyoe(word_type):\n",
    "    word_type = word_type.split(\"Basa\")\n",
    "    word_type = word_type[1:]\n",
    "    word_type = [i.replace(\"[ édit ]\", \":\") for i in word_type]\n",
    "    return word_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_mean(url):\n",
    "    links = []\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the <div> with class 'mw-category-group'\n",
    "    category_group_div = soup.find('div', id='bodyContent')\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    if category_group_div:\n",
    "        links = category_group_div.find_all('li')\n",
    "    #remove the tag from the list\n",
    "    links = [link.text for link in links]\n",
    "    return links\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(url):\n",
    "    word_type = extract_word_type(url)\n",
    "    word_type = clean_word_tyoe(word_type)  # Corrected function name\n",
    "    word_meaning = extract_word_mean(url)  # Corrected function name\n",
    "\n",
    "    # Use ':' instead of '=' in dictionary creation\n",
    "    content = {\n",
    "        'jenis kata': word_type,\n",
    "        'arti kata': word_meaning\n",
    "    }\n",
    "\n",
    "    return content  # Return the content dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_list:\n",
    "    url = word['link']\n",
    "    content = extract_content(url)\n",
    "    word.update(content)  # Update the word dictionary with the extracted content\n",
    "    print(word)  # Print the updated word dictionary\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n"
     ]
    }
   ],
   "source": [
    "#save the data to json\n",
    "print(len(word_list))\n",
    "with open('word_list_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(word_list, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load word_list_data.json\n",
    "with open('word_list_data.json', 'r', encoding='utf-8') as json_file:\n",
    "    word_list = json.load(json_file)\n",
    "\n",
    "\n",
    "# List of sentences to check and replace\n",
    "sentences_to_replace = ['Kecap bilangan', 'Kecap barang', 'Kecap pagawéan', 'Kecap Sipat', \n",
    "                        'Kecap panambah', 'Kecap panyeluk', 'Kecap panyambung', \n",
    "                        \"Kecap sulur\", \"Kecap pananya\", \"Kecap panuduh\", \"Kecap pangantét\", \"Kecap sipat\",\n",
    "                        \"Kecap Barang\"]\n",
    "\n",
    "# Iterate over each word in the word_list\n",
    "for word in word_list:\n",
    "    # Check if \"jenis kata\" is present in the word dictionary\n",
    "    if 'jenis kata' in word:\n",
    "        # Iterate over each sentence in the \"jenis kata\" list\n",
    "        for i in range(len(word['jenis kata'])):\n",
    "            for sentence in sentences_to_replace:\n",
    "                # Check if the sentence is present in the current \"jenis kata\" sentence\n",
    "                if sentence in word['jenis kata'][i]:\n",
    "                    # Replace the sentence with the desired text\n",
    "                    word['jenis kata'][i] = sentence\n",
    "\n",
    "\n",
    "for word in word_list:\n",
    "    if len(word['word']) < 3:\n",
    "        word_list.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(word_list, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Kecap sifat',\n",
       " 'Kecap panuduh',\n",
       " 'Kecap Barang',\n",
       " 'Kecap panyambung',\n",
       " ' Sunda : ',\n",
       " 'Kecap pagawéan',\n",
       " 'Kecap bilangan',\n",
       " ' Sunda :',\n",
       " 'Kecap pananya',\n",
       " ' Kecap Kaayaan',\n",
       " 'Kecap pangantét',\n",
       " 'Kecap sipat',\n",
       " 'Kecap sulur',\n",
       " ' Sunda : Conto :',\n",
       " 'Kecap Sipat',\n",
       " 'Kecap panyeluk',\n",
       " ' Sunda : Tempo ogé :',\n",
       " 'Kecap barang',\n",
       " 'Kecap panambah']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find every unique word type\n",
    "word_type_unique = []\n",
    "word_type_unique = [word['jenis kata'] for word in word_list]\n",
    "word_type_unique = [item for sublist in word_type_unique for item in sublist]\n",
    "word_type_unique = list(set(word_type_unique))\n",
    "word_type_unique\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3185"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the data to json\n",
    "with open('word_list_data2.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(word_list, json_file, indent=4, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Time-Series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
